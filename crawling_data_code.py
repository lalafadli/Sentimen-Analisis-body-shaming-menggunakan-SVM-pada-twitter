# -*- coding: utf-8 -*-
"""crawling data code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZT85lnxXITRsLaCg3mVOVJslh6iZ7IlC

# **Crawling Data**

Crawling data disini menggunakan node.js dengan Twitter Crawler dari tweet-harvest. Kemudian untuk menghasilkan file csv nya menggunakan perintah pandas dari python.

1. - !pip install pandas: Ini adalah perintah dalam lingkungan Python yang digunakan untuk menginstal pustaka atau modul Python yang disebut "pandas". Pandas adalah sebuah perpustakaan open-source yang menyediakan struktur data dan analisis data yang mudah digunakan untuk bahasa pemrograman Python. -
!curl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -: Perintah ini menggunakan utilitas curl untuk mengunduh skrip instalasi dari https://deb.nodesource.com/setup_18.x dan kemudian menjalankan skrip tersebut menggunakan bash. Skrip ini mungkin bertujuan untuk menyiapkan repositori Node.js versi 18.x pada sistem Linux Anda. - !sudo apt-get install -y nodejs: Ini adalah perintah untuk menginstal Node.js pada sistem Linux Anda menggunakan manajer paket apt. Node.js adalah lingkungan runtime JavaScript yang memungkinkan Anda menjalankan JavaScript di sisi server. Perintah ini akan menginstal Node.js pada sistem Anda.
"""

!pip install pandas
!curl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -
!sudo apt-get install -y nodejs

"""2. - Proses penarikan. ubah huruf xxx menjadi yang diinginkan. Dan masukan token twitter yang anda punya. Untuk mencari token twitter ada di folder dengan filename mencari authotoken twitter."""

# Crawl Data

data = 'kerempeng.csv'
search_keyword = 'kerempeng lang:id until:2024-08-29 since:2024-05-01'
limit = 200

!npx --yes tweet-harvest@latest -o "{data}" -s "{search_keyword}" -l {limit} --token "abb4c26c7cb990c4c78648fae0948e126a6f0780"

import pandas as pd

file_path = f"tweets-data/{data}"
df = pd.read_csv(file_path)
df = df[['full_text', 'id_str', 'created_at', 'username', 'user_id_str', 'lang', 'location', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'tweet_url']]
df['created_at'] = pd.to_datetime(df['created_at'])
df.sort_values(by='created_at', ascending=True, inplace=True)

import re

def extract_mentions(text):
    mentions = re.findall(r'@(\w+)', text)
    return mentions

df['mentions'] = df['full_text'].apply(extract_mentions)
df['mention_count'] = df['mentions'].apply(len)
df['source'] = df['username']

def choose_target(mentions):
    if mentions:
        return mentions[0]
    else:
        return None

df['target'] = df['mentions'].apply(choose_target)

df.to_csv('kerempeng.csv', index=False, sep=";") # Ganti nama file yang akan disimpan
df