# -*- coding: utf-8 -*-
"""preprocessing data  .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvddeRk8D9bY2dFM_gytMTTFxGrL3R_Z

# **Preprocessing Data**

1. Import modul untuk pembersihan teks, tokenisasi, analisis teks, dan manipulasi data numerik jika diperlukan dalam proses preprocessing.
"""

import pandas as pd
import numpy as np
import nltk
import string
import re

"""2. Masukan file dataset yang akan diproses."""

def load_data():
    data_df = pd.read_csv('/content/all data.csv')
    return data_df

data = load_data()
data

#drop kolom yang tidak dibutuhkan
data.drop(data.columns[[1,2,4,5,6,7,8,9,10,11,12,13,14,15,16]], axis=1, inplace=True)

display(data)

"""3. Cleaning.
Disini ada dua tahapan cleaning; cleaning username, dan cleaning menghapus karakter tanda baca, emoticon, numerik, link dsb. Proses ini menggunakan loop dengan modul re.
"""

#menghapus username dalam tweet
def remove_pattern(Text, pattern):
    r = re.findall(pattern, str(Text))
    for i in r:
        Text = re.sub(i, '', str(Text))
    return Text
data['remove_user'] = np.vectorize(remove_pattern)(data['full_text'], "@[\w]*")
data.head()

#cleaning
def cleaning(Text):
    Text = re.sub(r'\$\w*', '', Text)         #digunakan untuk menghapus semua kata yang dimulai dengan tanda dolar ($) dan diikuti oleh karakter huruf, angka, atau garis bawah.      #
    Text = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', ' ', Text)     #untuk menghapus semua URL atau tautan web dari teks.
    Text = re.sub('&quot;'," ", Text)         #Digunakan untuk menggantikan setiap kemunculan `&quot;` dengan spasi kosong dalam kolom `text`.
    Text = re.sub(r"\d+", " ", str(Text))         #digunakan untuk menggantikan semua angka dalam teks yang disimpan dalam kolom `text` dengan spasi kosong.
    Text = re.sub(r"\b[a-zA-Z]\b", "", str(Text))     #digunakan untuk menghapus semua kata tunggal dalam teks yang disimpan dalam kolom `text`.
    Text = re.sub(r"[^\w\s]", " ", str(Text))         #digunakan untuk menggantikan semua karakter non-alphanumerik dan non-spasi dalam teks yang disimpan dalam variabel `content` dengan spasi kosong.
    Text = re.sub(r'(.)\1+', r'\1\1', Text)         #Digunakan untuk mengganti dua atau lebih karakter berulang dalam teks dengan hanya dua karakter yang berulang. Misalnya, jika terdapat karakter berulang "eeeee" dalam teks, maka akan digantikan dengan "ee".
    Text = re.sub(r"\s+", " ", str(Text))       #digunakan untuk menggantikan satu atau lebih spasi berturut-turut dalam teks
    Text = re.sub(r'#', '', Text)         #digunakan untuk menghapus semua tanda pagar (#) dalam teks
    Text = re.sub(r'[^a-zA-z0-9]', ' ', str(Text))    #Digunakan untuk menggantikan semua karakter non-alphanumerik dalam teks dengan spasi kosong, sehingga menghapus karakter-karakter tersebut dari teks dan mempertahankan hanya huruf (kapital dan kecil) serta angka.
    Text = re.sub(r'\b\w{1,2}\b', '', Text)     #digunakan untuk menghapus kata-kata dengan panjang satu atau dua karakter dalam teks
    Text = re.sub(r'\s\s+', ' ', Text)      #Digunakan untuk menggantikan dua atau lebih spasi berturut-turut dalam teks dengan satu spasi tunggal.
    Text = re.sub(r'^RT[\s]+', '', Text)        #menghapus RT
    Text = re.sub(r'^b[\s]+', '', Text)       #digunakan untuk menghapus spasi di awal teks
    Text = re.sub(r'^link[\s]+', '', Text)      #digunakan untuk menghapus string "link" yang diikuti oleh spasi di awal teks
    return Text

def remove_emoji(Text):
    emoji = re.compile("["
                        u"\U0001F600-\U0001F64F"  # emoticons
                        u"\U0001F300-\U0001F5FF"  # simbol & piktogram
                        u"\U0001F680-\U0001F6FF"  # transportasi & simbol peralatan
                        u"\U0001F1E0-\U0001F1FF"  # bendera negara
                        u"\U00002702-\U000027B0"  # simbol
                        u"\U000024C2-\U0001F251"  # emoji lainnya
                        "]+", flags=re.UNICODE)
    return emoji.sub(r'', Text)

data['cleaning'] = data['remove_user'].apply(cleaning)
data.head()

"""4. Remove Duplikat Data.
Tahapan untuk menghapus dataset yang sama dengan menetapkan 1 dataset utama.
"""

#remove data duplikat dari kolom cleaning
data.drop_duplicates(subset = "cleaning", keep = 'first', inplace = True)
data

"""5. Case Folding.
Mengubah semua huruf besar menjadi huruf kecil menggunakan .str.lower()

"""

#case folding - ubah jadi huruf kecil
data['case_folding'] = data['cleaning'].str.lower()
data.head()

"""6. Tokenisasi/Tokenized.
Mengubah data menjadi daftar kata-kata/list. Disini menggunakan Library dari NLTK dengan mengimport fungsi word_tokenize dari modul nltk.tokenize. Dan fungsi word_tokenize_wrapper digunakan dalam kode tersebut sebagai perantara (wrapper) yang memungkinkan kita menerapkan fungsi word_tokenize dari NLTK ke setiap baris teks.
"""

#tokenisasi - membagi kalimat jadi perkata (dipisah)
nltk.download('punkt')
from nltk.tokenize import word_tokenize

def word_tokenize_wrapper(Tweets):
    return word_tokenize(Tweets)

data['tokenisasi'] = data['case_folding'].apply(lambda x: word_tokenize_wrapper(x.lower()))
data.head()

"""7. Normalisasi.
Proses mengganti kalimat slang menjadi formal dengan menggunakan dan membaca dari kamus 'colloquial-indonesian-lexicon.csv'. Kamus ini berisi daftar kata-kata slang dan bentuk formalnya dalam bahasa Indonesia. Kemudian melakukan loop. Di dalam loop ini, setiap kata dalam teks dicek apakah ada dalam kamus dict_slang. Jika kata tersebut ada dalam kamus, maka kata slang akan diganti dengan bentuk formalnya sesuai dengan kamus. Jika kata tersebut tidak ada dalam kamus, kata tersebut tetap dipertahankan. **notes: untuk file 'colloquial-indonesian-lexicon.csv' mohon dimasukan kedalam notebook terlebih dahulu sebelum memproses preprocessing (*sama hal nya seperti memasukan file dataset csv yang akan diproses*)**
"""

#Normalisasi-menormalisasikan kata yang non formal menjadi formal sesuai dengan kamus colloquial-indonesian-lexicon
def normalization (Tweets):
  tweets_slang = pd.read_csv('colloquial-indonesian-lexicon.csv')
  dict_slang ={}
  for i in range(tweets_slang.shape[0]):
    dict_slang[tweets_slang["slang"][i]]=tweets_slang["formal"][i]

  drop_slang = []
  for teks in Tweets:
    normalisasi_teks = [dict_slang[word] if word in dict_slang.keys() else word for word in teks]
    drop_slang.append(normalisasi_teks)

  return drop_slang

data['normalisasi'] = normalization(data['tokenisasi'])
data.head()

"""8. Stopword Removal/Removing. Menghapus kata yang tidak memiliki makna penting. Disini menggunakan Library dari NLTK dengan modul stopword dalam bahasa indonesia. from nltk.corpus import stopwords: Kode ini mengimpor daftar stopwords (kata-kata pengisi) dalam bahasa Indonesia dari NLTK. Daftar ini akan digunakan untuk mengidentifikasi kata-kata yang harus dihapus dari teks. Dan untuk code list_stopwords_id.extend: ini merupakan list tambahakan untuk menghapus kata yang tidak ada makna selain yang disediakan dalam corpus nltk stopword."""

#Stopword removal - filtering/menghapus kata yang tidak ada didalam kamus corpus nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# Bahasa Indonesia stopwords
list_stopwords_id = stopwords.words('indonesian')
list_stopwords_id.extend(['yg', 'dg', 'rt', 'dgn', 'ny', 'gt', 'klo',
                       'kalo', 'amp', 'xbf', 'xad', 'xef',
                       'xe', 'ga', 'krn', 'nya', 'nih', 'sih',
                       'si', 'xc', 'tdk', 'tuh', 'utk', 'ya',
                       'jd', 'jgn', 'sdh', 'xae', 'n', 't',
                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',
                       '&', 'yah', 'no', 'je', 'xbb', 'xb', 'sch',
                       'injirrr', 'ah', 'oena', 'bu', 'eh', 'xac', 'xbc', 'xf', 'xa'])

list_stopwords = set(list_stopwords_id)

def stopwords_removal(Text):
    return [word for word in Text if word not in list_stopwords]

data['stopword_removal'] = data['normalisasi'].apply(stopwords_removal)
data.head()

"""9. Stemming. Proses mengahpus imbuhan. Disini menggunakan Library sastrawi dahalam bahasa indonesia untuk menghapus imbuhannya. Modul stemmer digunakan untuk membuat objek stemmer. Sementara StopWordRemoverFactory membuat objek remover stopwords yang akan digunakan dalam tahap penghapusan, disini kasusnya seperti mengurangi noise, seperti "dan", "atau", "di", "dari", dll. Modul swifter disini berfungsi untuk mempercepat proses stemming.


"""

#stemming - menghapus imbuhan
!pip install swifter
!pip install Sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
import swifter

#buat stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

#stemmed wrapper
def stemmed_wrapper(term):
  return stemmer.stem(term)

term_dict = {}

for Tweets in data['stopword_removal']:
  for term in Tweets:
    if term not in term_dict:
      term_dict[term] = ' '

print(len(term_dict))
print("------------------------")

for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    print(term,":" ,term_dict[term])

print(term_dict)
print("------------------------")

#memmulai stemming
def apply_stemmed_term(Tweets):
  return [term_dict[term] for term in Tweets]

data['stemming'] = data['stopword_removal'].swifter.apply(apply_stemmed_term)
data.head()

"""10. To Sentence. Mengubah bentuk list token pada kolom stemming menjadi sentence atau kalimat."""

#To Sentence/Untokenized
stemming = data[['stemming']]

def to_fit_sentence(Tweets):
    Tweets = np.array(Tweets)
    Tweets = ' '.join(Tweets)

    return Tweets

data['to_sentence'] = data['stemming'].apply(lambda x: to_fit_sentence(x))
data.head()

"""11. Save kedalam csv"""

#simpan kedalam csv
data.to_csv('preprocessing_data-gabungan_keyword.csv', sep=',', index=False)