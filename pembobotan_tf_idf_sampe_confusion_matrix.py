# -*- coding: utf-8 -*-
"""pembobotan tf idf sampe confusion matrix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uCd28jmdtKIzOkW1dtELCJ3JpiRURiqD
"""

import pandas as pd
import numpy as np
import nltk
import string
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

def load_data():
    #Corrected file name
    data = pd.read_csv('/content/labeling_sentimen-gabungan_keyword 2.csv_test.csv')
    return data

import pandas as pd

def load_data():
    #Corrected file name and path. Please make sure file exists at the given path.
    data = pd.read_csv('/content/labeling_sentimen-gabungan_keyword 2.csv')
    return data

# Call the load_data function to create the DataFrame
tweet_df = load_data()

# Now you can access the shape attribute
tweet_df.shape

#menghitung banyaknya data sesuai jenis label nya
tweet_df['sentiment'].value_counts()

#memunculkan data grafik berdasarkan label untuk menunjukkan keseimbangan data
# Import matplotlib.pyplot for plotting
import matplotlib.pyplot as plt

# Import Counter for counting label occurrences
from collections import Counter

# Use 'tweet_df' instead of 'df'
label_cnt = Counter(tweet_df.sentiment)

plt.figure(figsize=(16,8))
plt.bar(label_cnt.keys(), label_cnt.values())
plt.title("Dataset labels distribuition")

# Convert text to lowercase
tweet_df['lower'] = tweet_df['text'].str.lower()

#Mengurutkan ascending urutkan kolom tweet
tweet_df.sort_values("to_sentence", inplace = True)

tweet_df.head()

import string
tweet_df['sentiment'].value_counts()

# Handle missing values before fitting CountVectorizer
tweet_df['to_sentence'] = tweet_df['to_sentence'].fillna('')  # Replace NaN with empty strings

from sklearn.feature_extraction.text import CountVectorizer
bow_transformer = CountVectorizer().fit(tweet_df['to_sentence'])
bow_transformer.vocabulary_

tokens = bow_transformer.get_feature_names_out()
print(tokens)

text_bow = bow_transformer.transform(tweet_df['to_sentence'])
print(text_bow)

import string
from sklearn.feature_extraction.text import CountVectorizer

# Handle missing values before fitting CountVectorizer
tweet_df['to_sentence'] = tweet_df['to_sentence'].fillna('')  # Replace NaN with empty strings

bow_transformer = CountVectorizer().fit(tweet_df['to_sentence'])
# print(bow_transformer.vocabulary_) # Optionally print vocabulary

# Get feature names
tokens = bow_transformer.get_feature_names_out()
print(tokens)

# Transform the data
text_bow = bow_transformer.transform(tweet_df['to_sentence']) # This line creates 'text_bow'
print(text_bow)

# Now you can use text_bow
X = text_bow.toarray()
print(X)
X.shape

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer=TfidfTransformer().fit(text_bow)
print(tfidf_transformer)

tweet_tfidf=tfidf_transformer.transform(text_bow)
print(tweet_tfidf)
print(tweet_tfidf.shape)

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, tweet_df.sentiment,test_size=0.2, random_state=35)

def model(model, data_train, y_train, data_test, y_test):
    model.fit(data_train, y_train)
    predict = model.predict(data_test)
    cm = confusion_matrix(y_test, predict, labels=labels)
    accuracy = round(accuracy_score(y_test, predict) * 100, 2)
    # Classification report
    target_names = ['Negatif', 'Netral', 'Positif']
    cr = classification_report(y_test, predict, target_names=target_names)

    hasil = pd.DataFrame(X_test, columns=['Tweet'])
    hasil['Aktual'] = y_test
    hasil['Aktual'] = hasil['Aktual'].map({ 0:"Netral", 1:"Positif", -1:"Negatif" })
    hasil['Predict'] = predict
    hasil['Predict'] = hasil['Predict'].map( {0:"Netral", 1:"Positif", -1:"Negatif"})

    return hasil, predict, cm, accuracy, cr

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

x_train_tfidf, x_test_tfidf, y_train, y_test = train_test_split(tweet_tfidf, tweet_df.sentiment, test_size=0.2, random_state=35)

svm_model = SVC(kernel='linear')
svm_model.fit(x_train_tfidf, y_train)

y_pred = svm_model.predict(x_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report")
print(classification_report(y_test, y_pred))

print(svm_model.score(x_test, y_test))

from sklearn.metrics import classification_report

pred = svm_model.predict(x_test)
print(classification_report(y_test, pred))

from time import time
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from pandas import DataFrame


t = time()
# Replace 'model' with 'svm_model'
y_pred = svm_model.predict(x_test)

test_time = time() - t
print("test time:  %0.3fs" % test_time)

score1 = metrics.accuracy_score(y_test, y_pred)
print("accuracy:   %0.3f" % score1)

# Add 'Neutral' to the target_names list
print(metrics.classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))

# Update the columns for the confusion matrix
columns = ['Negative', 'Neutral', 'Positive']
confm = confusion_matrix(y_test, y_pred)
df_cm = DataFrame(confm, index=columns, columns=columns)

ax = sn.heatmap(df_cm, cmap='Greens', annot=True)
ax.set_title('Confusion matrix')
ax.set_xlabel('Label prediksi')

pip install wordcloud matplotlib nltk

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import nltk

# Unduh stopwords NLTK
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Contoh data teks (bisa diganti dengan data ulasan, artikel, atau dataset lain)
text_data = " ".join(tweet_df['to_sentence'])

# Membersihkan teks (hapus stopwords)
filtered_text = " ".join(word for word in text_data.split() if word.lower() not in stop_words)

# Membuat WordCloud
wordcloud = WordCloud(
    width=800,  # Lebar gambar
    height=400,  # Tinggi gambar
    background_color='white',  # Warna latar belakang
    colormap='viridis',  # Warna tema
    stopwords=stop_words,  # Stopwords tambahan
    max_words=100,  # Jumlah maksimal kata yang akan divisualisasikan
    contour_color='steelblue',  # Warna kontur
    contour_width=1  # Ketebalan kontur
).generate(filtered_text)

# Plot WordCloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Matikan sumbu
plt.title("Word Cloud Visualization", fontsize=16)
plt.show()

import matplotlib.pyplot as plt
from collections import Counter
import string

# Daftar custom stopwords
custom_stopwords = {"gue", "banget", "kayak", "orang", "pakai", "badan", "anak", "pas", "deh", "lihat", "suka", "hidung", "makan", "muka", "tau",
                    "gua", "kulit", "cowok", "bikin", "lucu", "enggak", "coba", "main", "rambut", "gede", "bilang", "pria", "biar", "mah", "anjir",
                    "pengin", "iya", "kali", "bekas", "teman", "habis"}


text_data = " ".join(tweet_df['to_sentence'])

# Fungsi membersihkan teks dengan stopwords
def clean_text_custom(text, stopwords):
    text = text.lower()  # Mengubah ke huruf kecil
    text = text.translate(str.maketrans('', '', string.punctuation))  # Hapus tanda baca
    words = [word for word in text.split() if word not in stopwords]  # Hapus stopwords
    return words

# Pembersihan teks
cleaned_words = clean_text_custom(text_data, custom_stopwords)

# Menghitung frekuensi kata
word_freq = Counter(cleaned_words)
most_common_words = word_freq.most_common(5)  # Ambil kata paling sering muncul

# Memisahkan kata dan frekuensinya
words, counts = zip(*most_common_words)

# Membuat diagram batang
plt.figure(figsize=(10, 6))
bars = plt.bar(words, counts, color='skyblue')

# Menambahkan label frekuensi di atas setiap batang
for bar, count in zip(bars, counts):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() - 0.5,
             str(count), ha='center', va='bottom', fontsize=10, color='black')

# Judul dan label diagram
plt.title("5 kata body shaming terbanyak", fontsize=14)
plt.xlabel("Kata", fontsize=12)
plt.ylabel("Frekuensi", fontsize=12)
plt.xticks(rotation=45)  # Memiringkan label pada sumbu X
plt.show()